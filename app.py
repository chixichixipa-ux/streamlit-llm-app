import os
from dotenv import load_dotenv
load_dotenv()

import streamlit as st
from openai import OpenAI
import json
from datetime import datetime

st.set_page_config(
    page_title="ğŸ¤– AI ãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []
if "total_tokens" not in st.session_state:
    st.session_state.total_tokens = 0

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.title("âš™ï¸ è¨­å®š")
    
    st.subheader("ãƒ¢ãƒ‡ãƒ«é¸æŠ")
    model = st.selectbox(
        "AIãƒ¢ãƒ‡ãƒ«",
        ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
        index=0,
        help="ä½¿ç”¨ã™ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„"
    )
    
    st.subheader("ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´")
    temperature = st.slider(
        "Temperatureï¼ˆå‰µé€ æ€§ï¼‰",
        min_value=0.0,
        max_value=2.0,
        value=0.7,
        step=0.1,
        help="å€¤ãŒé«˜ã„ã»ã©å‰µé€ çš„ã§äºˆæ¸¬ä¸å¯èƒ½ãªå›ç­”ã«ãªã‚Šã¾ã™"
    )
    
    max_tokens = st.slider(
        "æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°",
        min_value=100,
        max_value=4000,
        value=1000,
        step=100,
        help="ç”Ÿæˆã•ã‚Œã‚‹å›ç­”ã®æœ€å¤§é•·"
    )
    
    st.subheader("ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
    system_prompt = st.text_area(
        "AIã®å½¹å‰²ã‚’è¨­å®š",
        value="ã‚ãªãŸã¯è¦ªåˆ‡ã§çŸ¥è­˜è±Šå¯ŒãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€æ­£ç¢ºã§åˆ†ã‹ã‚Šã‚„ã™ã„æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚",
        height=150,
        help="AIã®æŒ¯ã‚‹èˆã„ã‚„å½¹å‰²ã‚’è¨­å®šã§ãã¾ã™"
    )
    
    st.divider()
    
    # çµ±è¨ˆæƒ…å ±
    st.subheader("ğŸ“Š çµ±è¨ˆæƒ…å ±")
    st.metric("ä¼šè©±ã®ã‚„ã‚Šå–ã‚Šæ•°", len(st.session_state.messages) // 2)
    st.metric("ç´¯è¨ˆãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡", st.session_state.total_tokens)
    
    st.divider()
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ç®¡ç†
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ å±¥æ­´ã‚¯ãƒªã‚¢", use_container_width=True):
            st.session_state.messages = []
            st.session_state.total_tokens = 0
            st.rerun()
    
    with col2:
        if st.button("ğŸ’¾ å±¥æ­´ä¿å­˜", use_container_width=True):
            if st.session_state.messages:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"chat_history_{timestamp}.json"
                chat_data = {
                    "timestamp": timestamp,
                    "model": model,
                    "messages": st.session_state.messages
                }
                st.download_button(
                    label="ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                    data=json.dumps(chat_data, ensure_ascii=False, indent=2),
                    file_name=filename,
                    mime="application/json",
                    use_container_width=True
                )

# ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
st.title("ğŸ¤– AI ãƒãƒ£ãƒƒãƒˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
st.markdown("---")

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
chat_container = st.container()
with chat_container:
    for message in st.session_state.messages:
        with st.chat_message(message["role"], avatar="ğŸ§‘â€ğŸ’»" if message["role"] == "user" else "ğŸ¤–"):
            st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„..."):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
        st.markdown(prompt)
    
    # AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        message_placeholder = st.empty()
        full_response = ""
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        with st.spinner("è€ƒãˆä¸­..."):
            try:
                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ§‹ç¯‰
                messages = [{"role": "system", "content": system_prompt}]
                messages.extend(st.session_state.messages)
                
                stream = client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content is not None:
                        full_response += chunk.choices[0].delta.content
                        message_placeholder.markdown(full_response + "â–Œ")
                
                message_placeholder.markdown(full_response)
                
                # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®æ›´æ–°ï¼ˆæ¦‚ç®—ï¼‰
                st.session_state.total_tokens += len(prompt.split()) + len(full_response.split())
                
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                full_response = "ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "assistant", "content": full_response})

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown(
    """
    <div style='text-align: center; color: gray; font-size: 0.8em;'>
    ğŸ’¡ ãƒ’ãƒ³ãƒˆ: ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã§ãã¾ã™
    </div>
    """,
    unsafe_allow_html=True
) 
